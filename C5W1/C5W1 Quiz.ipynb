{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30652b2d",
   "metadata": {},
   "source": [
    "1. 10000 단어의 어휘에 대한 단어 임베딩을 학습한다고 가정하십시오. 그러면 임베딩 벡터는 해당 단어의 전체 범위와 의미를 포착하기 위해 10000 차원이어야 합니다.  \n",
    "답: False\n",
    "\n",
    "2. t-SNE는 무엇입니까?\n",
    "* 단어 벡터에 대한 유추를 풀 수 있는 선형 변환\n",
    "* (0) 비선형 차원 감소 기술\n",
    "* 단어 임베딩 학습을 위한 지도학습 알고리즘\n",
    "* 오픈 소스 시퀀스 모델링 라이브러리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b3a605",
   "metadata": {},
   "source": [
    "3. 방대한 텍스트 코퍼스에서 훈련된 사전 훈련된 단어 임베딩을 다운로드한다고 가정 해보십시오. 그러면, 이 단어 임베딩을 사용하여 작은 학습 세트를 사용하여 누군가가 짧은 텍스트 스니펫에서 만족하는지 인식하는 언어 작업을 위해 RNN을 학습시킵니다.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>x(input text)</th>\n",
    "        <th>y(happy?)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> I'm feeling wonderfun today!</td>\n",
    "        <td> 1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> I'm bimmed my cat is ill</td>\n",
    "        <td> 0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Really enjoying this!</td>\n",
    "        <td> 1</td>\n",
    "    </tr>\n",
    "</table>\n",
    "그런 다음 작은 학습 세트에 \"ecstatic\"이라는 단어가 나타나지 않더라도 RNN은 레이블 y = 1에 해당하는 \"I'm ecstatic\"를 합리적으로 인식 할 것으로 예상 할 수 있습니다.\n",
    "\n",
    "ANS: False\n",
    "\n",
    "4. 좋은 단어 임베딩을 위해 다음 방정식 중 어떤 것이 유지되어야 한다고 생각하십니까? (해당되는 모든 것을 체크하세요)\n",
    "\n",
    "* O $e_{boy} - e_{girl} \\approx e_{brother} - e_{sister}$\n",
    "* $e_{boy} - e_{girl} \\approx e_{sister} - e_{brother}$\n",
    "* O $e_{boy} - e_{brother} \\approx e_{girl} - e_{sister}$\n",
    "* $e_{boy} - e_{brother} \\approx e_{sister} - e_{girl}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b83cb",
   "metadata": {},
   "source": [
    "5. E 를 임베딩 행렬로 하고 $o_{1234}$를 단어 1234에 해당하는 원-핫 벡터라고 합니다. 그런 다음 단어 1234의 임베딩을 얻기 위해 Python에서 $E * o_{1234}$를 호출하지 않는 이유는 무엇입니까?\n",
    "\n",
    "* O 계산적으로 낭비입니다.\n",
    "* 올바른 공식은 $E^T * o_{1234}$입니다.\n",
    "* 이것은 알 수 없는 단어(&lt;UNK&gt;)는 처리하지 않습니다.\n",
    "* 해당 사항 없음 : 위에서 설명한대로 Python 부분을 호출하는 것이 좋습니다.\n",
    "\n",
    "6. 단어 임베딩을 학습 할 때 P(target context)를 추정하는 인위적인 작업을 만듭니다. 이 인공적인 예측 작업을 제대로 수행하지 않아도 괜찮습니다. 이 작업의 더 중요한 부산물은 유용한 단어 임베딩 세트를 학습한다는 것입니다.\n",
    "ANS : True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63d9c7",
   "metadata": {},
   "source": [
    "7. word2vec 알고리즘에서 $P(t|c)$를 추정합니다. 여기서 $t$는 대상 단어이고 $c$는 컨텍스트 단어입니다. 훈련 세트에서 $t$와 $c$는 어떻게 선택됩니까? 가장 좋은 답변을 선택하세요.\n",
    "\n",
    "* c 는 t 앞의 문장에서 모든 단어의 시퀀스입니다.\n",
    "* c 는 t 바로 앞에 오는 한 단어입니다.\n",
    "* (0) c 와 t 는 근처 단어로 선택됩니다.\n",
    "* c 는 t 바로 앞의 여러 단어 시퀀스입니다.\n",
    "\n",
    "8. 10000 개의 단어 어휘가 있고 500 차원 단어 임베딩을 배우고 있다고 가정합니다. word2vec 모델은 다음과 같은 softmax 함수를 사용합니다.\n",
    "\n",
    "$P(t|c) = { e^{\\theta_t^T e_c} \\over \\sum_{t'=1}^{10000} e^{\\theta_{t'}^T e_c}}$\n",
    "\n",
    "다음 중 올바른 설명은 무엇입니까? 해당되는 모든 것을 체크하세요.\n",
    "\n",
    "* (0) $\\theta_t$ 과 $e_c$ 는 모두 500 차원 벡터입니다.\n",
    "* $\\theta_t$ 과 $e_c$는 모두 10000 차원 벡터입니다.\n",
    "* (0) $\\theta_t$ 과 $e_c$는 모두 Adam 또는 경사하강법과 같은 최적화 알고리즘으로 훈련되었습니다.\n",
    "* 훈련 후에, $t$와 $c$가 같은 단어 일때, 우리는 $\\theta_t$ 가 $e_c$ 에 매우 가까울 걸 기대합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d406cf",
   "metadata": {},
   "source": [
    "9. 10000 개의 단어 어휘가 있고 500 차원 단어 임베딩을 배우고 있다고 가정합니다. Glove 모델은 다음의 목표를 최소화합니다.\n",
    "\n",
    "$\\min \\sum_{i=1}^{10,000} \\sum_{j=1}^{10,000} f(X_{ij}(\\theta_i^T e_j + b_i + b_j' - \\log X_{ij})^2$\n",
    "\n",
    "다음 중 올바른 설명은 무엇입니까? 해당되는 모든 것을 체크하세요.\n",
    "\n",
    "* $\\theta_i$과 $e_j$는 훈련 시작시 0으로 초기화되어야 합니다.\n",
    "* (0) $\\theta_i$과 $e_j$는 훈련 시작시 무작위로 초기화되어야 합니다.\n",
    "* (0) $X_{ij}$는 단어 i가 단어의 문맥에 나타나는 횟수입니다.\n",
    "* (0) 가중치 함수 $f(.)$ 는 $f(0)=0$을 충족해야 합니다.\n",
    "\n",
    "10. $m_1$ 단어의 텍스트 데이터 세트를 사용하여 단어 임베딩을 훈련했습니다. 당신은 $m_2$ 단어로 구성된 별도의 레이블이 지정된 데이터 세트가 있는 언어 작업에 이러한 단어 임베딩 사용을 고려합니다. 단어 임베딩을 사용하는 것이 전이 학습의 한 형태라는 점을 생각한다면, 다음의 어떤 상황에서 단어 임베딩이 도움이 될 것으로 기대합니까?\n",
    "\n",
    "* (0) $m_1 >> m_2$\n",
    "* $m_1 << m_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27457f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
